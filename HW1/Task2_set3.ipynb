{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly\n",
        "!pip install sklearn_crfsuite\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OelbMV2okLi",
        "outputId": "0eb777c8-66a1-4f05-bf01-edc9db1693f8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "V1jw7V1ToIm0"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "import string\n",
        "import random\n",
        "import pickle\n",
        "import numpy\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHWKVpEtoK-d",
        "outputId": "2e187d35-dee5-4989-c762-5663c03d229b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatdata(formatted_sentences,formatted_labels,file_name):\n",
        "\t#file=open(\"en-ud-dev.conllu\",\"r\")\n",
        "\tfile=open(file_name, 'r', encoding='ascii', errors='backslashreplace')\n",
        "\t#file=open(file_name,\"rb\")\n",
        "\tprint(\"Reading data...\")\n",
        "\t#quit()\n",
        "\ttext=file.read().splitlines()\n",
        "\ttokens=[]\n",
        "\tlabels=[]\n",
        "\tfor line in text:\n",
        "\t\tline=line.split('\\t')\n",
        "\t\tif len(line)==3:\n",
        "\t\t\ttokens.append(line[0])\n",
        "\t\t\tif line[1]==\"PUNCT\":\n",
        "\t\t\t\tlabels.append(line[0]+\"P\")\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabels.append(line[2])\n",
        "\t\telse:\n",
        "\t\t\tformatted_sentences.append(tokens)\n",
        "\t\t\tformatted_labels.append(labels)\n",
        "\t\t\ttokens=[]\n",
        "\t\t\tlabels=[]"
      ],
      "metadata": {
        "id": "s8v3BfWQxEDU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def creatdict(sentence,index,pos):\t#pos==\"\" <-> featuresofword  else, relative pos (str) is pos\n",
        "  word=sentence[index]\n",
        "  wordlow=word.lower()\n",
        "\n",
        "  suffix = wordlow[-3:]\n",
        "  suffix_last2 = wordlow[-2:]\n",
        "  suffix_last1 = wordlow[-1]\n",
        "\n",
        "  suffix_prev1 = \"START\"\n",
        "  suffix_prev2 = \"START\"\n",
        "  suffix_prev3 = \"START\"\n",
        "  suffix_next1 = \"END\"\n",
        "  suffix_next2 = \"END\"\n",
        "  suffix_next3 = \"END\"\n",
        "\n",
        "  prevword=\"START\"\n",
        "  prevwordlow=\"START\"\n",
        "\n",
        "  nextword=\"END\"\n",
        "  nextwordlow=\"END\"\n",
        "\n",
        "  if index != 0:\n",
        "    prevword=sentence[index-1]\n",
        "    prevwordlow=prevword.lower()\n",
        "    suffix_prev1 = prevwordlow[-1]\n",
        "    suffix_prev2 = prevwordlow[-2:]\n",
        "    suffix_prev3 = prevwordlow[-3:]\n",
        "\n",
        "  if index != len(sentence)-1:\n",
        "    nextword=sentence[index+1]\n",
        "    nextwordlow=nextword.lower()\n",
        "    suffix_next1 = nextwordlow[-1]\n",
        "    suffix_next2 = nextwordlow[-2:]\n",
        "    suffix_next3 = nextwordlow[-3:]\n",
        "\n",
        "\n",
        "  dict3={\n",
        "    \"wrd\"+pos:wordlow,\t\t\t\t\t\t\t\t# the token itself\n",
        "    \"cap\"+pos:word[0].isupper(),\t\t\t\t\t# starts with capital?\n",
        "    \"allcap\"+pos:word.isupper(),\t\t\t\t\t# is all capitals?\n",
        "    \"caps_inside\"+pos:word==wordlow,\t\t\t\t# has capitals inside?\n",
        "    \"nums?\"+pos:any(i.isdigit() for i in word),\t\t# has digits?\n",
        "    \"wrd-1\"+pos:prevwordlow,\t\t\t\t\t\t\t# previous token\n",
        "    \"suffix\" + pos: suffix,\n",
        "    \"suffix_last2\"+pos: suffix_last2,\n",
        "    \"suffix_last1\"+pos: suffix_last1,\n",
        "    \"nums?prev\"+pos:any(i.isdigit() for i in prevwordlow),\t\t# has digits?\n",
        "    \"nums?next\"+pos:any(i.isdigit() for i in nextwordlow),\t\t# has digits?\n",
        "    # \"suffix_prev_last1\"+pos: suffix_prev1,\n",
        "    # \"suffix_prev_last2\"+pos: suffix_prev2,\n",
        "    # \"suffix_prev_last3\"+pos: suffix_prev3,\n",
        "    # \"suffix_next_last1\"+pos: suffix_next1,\n",
        "    # \"suffix_next_last2\"+pos: suffix_next2,\n",
        "    # \"suffix_next_last3\"+pos: suffix_next3,\n",
        "\n",
        "\n",
        "  }\n",
        "  return dict3\n",
        "\n",
        "\n",
        "\n",
        "def feature_extractor(sentence,index):\n",
        "\n",
        "  features = creatdict(sentence,index,\"\")\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "7-54SfTuxFna"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creatsets(file_name):\n",
        "  sentences=[]\n",
        "  labels=[] \t#y_train (will be)\n",
        "  formatdata(sentences,labels,file_name)\n",
        "  limit=int(len(sentences))##############**********CHANGE these. these just limit the size of training set for faster trials. #####################\n",
        "  sentences=sentences[:limit]##############\n",
        "  labels=labels[:limit]####################\n",
        "\n",
        "  print(len(sentences),len(labels))\n",
        "  # print(sentences)\n",
        "  # print(labels)\n",
        "  print(\"Feature extraction...\")\n",
        "  features=[]\t\t#X_train\n",
        "  for i in range(0,len(sentences)):\n",
        "    features.append([])\n",
        "    for j in range(0,len(sentences[i])):\n",
        "      features[-1].append(feature_extractor(sentences[i],j))\n",
        "\n",
        "\n",
        "  del sentences[:]\n",
        "  del sentences\n",
        "\n",
        "\n",
        "  delimit=int((len(labels)*8)/10)\n",
        "  test_data=[features[delimit:],labels[delimit:]]\n",
        "  features=features[:delimit]\n",
        "  labels=labels[:delimit]\n",
        "\n",
        "  training_data=[features,labels]\n",
        "\n",
        "\n",
        "  delimit=int((len(labels)*8)/10)\n",
        "  test_data=[features[delimit:],labels[delimit:]]\n",
        "  features=features[:delimit]\n",
        "  labels=labels[:delimit]\n",
        "\n",
        "  training_data=[features,labels]\n",
        "\n",
        "\n",
        "  with open('pos_crf_train.data', 'wb') as file:\n",
        "    pickle.dump(training_data, file)\n",
        "  file.close()\n",
        "\n",
        "\n",
        "  with open('pos_crf_test.data', 'wb') as file:\n",
        "    pickle.dump(test_data, file)\n",
        "  file.close()\n",
        "\n",
        "  return training_data, test_data"
      ],
      "metadata": {
        "id": "NDIEZPSwxI-Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zvg8AG94oIm2"
      },
      "outputs": [],
      "source": [
        "def train(training_data):\n",
        "\tprint(\"Training...\")\n",
        "\tfeatures=training_data[0]\n",
        "\tlabels=training_data[1]\n",
        "\tclassifier.fit(features,labels)\n",
        "\n",
        "\n",
        "def test(test_data):\n",
        "\tprint(\"Testing...\")\n",
        "\n",
        "\ty_true=test_data[1]  #labels\n",
        "\ty_pred=classifier.predict(test_data[0])\n",
        "\n",
        "\t#print(y_pred[0])\n",
        "\n",
        "\tprecision=sklearn_crfsuite.metrics.flat_precision_score(y_true, y_pred,average='micro')\n",
        "\trecall=sklearn_crfsuite.metrics.flat_recall_score(y_true, y_pred,average='micro')\n",
        "\tf1=2*(precision*recall)/(precision+recall)\n",
        "\taccuracy=sklearn_crfsuite.metrics.flat_accuracy_score(y_true, y_pred)\n",
        "\n",
        "\tprint(\"accuracy:\",accuracy)\n",
        "\tprint(\"f1:\",f1)\n",
        "\tprint(\"precision:\",f1)\n",
        "\tprint(\"recall:\",recall)\n",
        "\n",
        "\n",
        "\n",
        "\tflat_y_true=[]\n",
        "\tflat_y_pred=[]\n",
        "\n",
        "\tfor x in y_true:\n",
        "\t\tfor y in x:\n",
        "\t\t\tflat_y_true.append(y)\n",
        "\n",
        "\tfor x in y_pred:\n",
        "\t\tfor y in x:\n",
        "\t\t\tflat_y_pred.append(y)\n",
        "\n",
        "\tend_p=[\"RP\",\"NFP\",\"VBP\",\"NNP\",\"PRP\",\"WP\"]\n",
        "\tfor i in range(0,len(flat_y_true)):\n",
        "\t\tif flat_y_true[i][-1]==\"P\" and flat_y_true[i][-1] not in end_p:\n",
        "\t\t\tflat_y_true[i]=\"PUNCT\"\n",
        "\t\tif flat_y_pred[i][-1]==\"P\" and flat_y_pred[i][-1] not in end_p:\n",
        "\t\t\tflat_y_pred[i]=\"PUNCT\"\n",
        "\n",
        "\t#print(type(flat_y_true))\n",
        "\t#print(flat_y_true[0],flat_y_true[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save(filename):\t#filename shall end with .pickle and type(filename)=string\n",
        "\tprint(\"Saving classifier.\")\n",
        "\twith open(filename, \"wb\") as f:\n",
        "\t\tpickle.dump(classifier, f)\n",
        "\treturn\n",
        "\n",
        "\n",
        "def load(filename):\t#filename shall end with .pickle and type(filename)=string\n",
        "\tprint(\"Loading classifier...\")\n",
        "\twith open(filename, \"rb\") as f:\n",
        "\t\tclassifier=pickle.load(f)\n",
        "\t\treturn classifier"
      ],
      "metadata": {
        "id": "AC4xcQ_-xbAH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tag(sentence):\n",
        "\t#takes a single sentence as a list\n",
        "\tclassifier=load(\"pos_crf.pickle\")\n",
        "\tt_features=[]\n",
        "\tfor j in range(0,len(sentence)):\n",
        "\t\tt_features.append(feature_extractor(sentence,j))\n",
        "\n",
        "\t#print(sentence)\n",
        "\t#print(len(t_features))\n",
        "\n",
        "\tret=classifier.predict([t_features])[0]\n",
        "\tend_p=[\"RP\",\"NFP\",\"VBP\",\"NNP\",\"PRP\",\"WP\"]\n",
        "\tfor i in range(0,len(ret)):\n",
        "\t\tif ret[i][-1]==\"P\" and ret[i][-1] not in end_p:\n",
        "\t\t\tret[i]=\"PUNCT\"\n",
        "\n",
        "\treturn ret"
      ],
      "metadata": {
        "id": "m7RUhmNZxdYY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvykoyZoIm3",
        "outputId": "c1577fd5-04f3-4328-cf64-f3d330be7d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            "12543 12543\n",
            "Feature extraction...\n",
            "Training...\n",
            "Saving classifier.\n",
            "Loading classifier...\n",
            "Testing...\n",
            "accuracy: 0.9270466877347803\n",
            "f1: 0.9270466877347803\n",
            "precision: 0.9270466877347803\n",
            "recall: 0.9270466877347803\n",
            "Loading classifier...\n",
            "['DT', 'NN', 'VBD', 'IN', 'DT', 'NNS', 'PUNCT', 'IN', 'CD', 'VBD', 'CD', 'PUNCT']\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tclassifier=sklearn_crfsuite.CRF(c1=0.2, c2=0.2, max_iterations=1000)\n",
        "\tbasedir = \"/content/drive/MyDrive/NLP_HW1_data/\"\n",
        "\ttraining_data, test_data=creatsets(basedir + \"en-ud-train.conllu\")\n",
        "\n",
        "\twith open('pos_crf_train.data', 'rb') as file:\n",
        "\t\ttraining_data=pickle.load(file)\n",
        "\tfile.close()\n",
        "\n",
        "\n",
        "\ttrain(training_data)\n",
        "\t#train(training_data2)\n",
        "\t#quit()\n",
        "\tsave(\"pos_crf.pickle\")\n",
        "\n",
        "\n",
        "\twith open('pos_crf_test.data', 'rb') as file:\n",
        "\t\ttest_data=pickle.load(file)\n",
        "\tfile.close()\n",
        "\n",
        "\tclassifier=load(\"pos_crf.pickle\")\n",
        "\ttest(test_data)\n",
        "\t#test(test_data2)\n",
        "\n",
        "\ts=['The',\n",
        "\t'guitarist',\n",
        "\t'died',\n",
        "\t'of',\n",
        "\t'a',\n",
        "\t'drugs',\n",
        "\t'overdose',\n",
        "\t'in',\n",
        "\t'1970',\n",
        "\t'aged',\n",
        "\t'27',\n",
        "\t'.']\n",
        "\n",
        "\tprint(tag(s))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}